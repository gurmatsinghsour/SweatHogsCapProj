{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EtHJnDFcNj9",
        "outputId": "5ba805b4-0ed1-4531-d1de-c778f3ca95dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deap in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (1.4.3)\n",
            "Requirement already satisfied: optuna in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (4.4.0)\n",
            "Requirement already satisfied: numpy in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from deap) (2.1.3)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from optuna) (1.16.2)\n",
            "Requirement already satisfied: colorlog in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Libraries installed and imported successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/paul/SweatHogsCapProj/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install deap optuna\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from deap import base, creator, tools, algorithms\n",
        "import optuna\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Suppress Optuna's trial logs for cleaner output during optimization\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "print(\"Libraries installed and imported successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDbXSvUXcv_E",
        "outputId": "7955c479-b123-4222-aaf4-98e9e7508a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data from medical_data.pkl\n",
            "\n",
            "Data loaded and preprocessed.\n",
            "Training set shape: (62439, 183)\n",
            "Test set shape: (15610, 183)\n",
            "Number of numeric features: 173\n",
            "Number of categorical features: 10\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "# We'll try to load a pre-processed pickle file first for speed.\n",
        "# If it doesn't exist, we load the CSV and create the pickle file.\n",
        "try:\n",
        "    df = pd.read_pickle('../../dataPreprocessing/medical_data.pkl')\n",
        "    print(\"Loaded data from medical_data.pkl\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Info: 'medical_data.pkl' not found. Attempting to load from CSV.\")\n",
        "\n",
        "# --- Data Cleaning and Preparation ---\n",
        "# Replace '?' with NaN and handle potential errors\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "\n",
        "# Define target and features\n",
        "TARGET = 'readmitted_ind'\n",
        "X = df.drop(TARGET, axis=1)\n",
        "y = df[TARGET]\n",
        "\n",
        "# Encode the target variable to be numeric (0 and 1)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# --- Feature Definition ---\n",
        "# Define features to exclude based on your example\n",
        "exclude_features = [\n",
        "    'patient_nbr', 'encounter_id', 'diagnosis_tuple', 'readmitted', 'dummy'\n",
        "]\n",
        "\n",
        "# Filter out excluded columns that are not in the dataframe\n",
        "existing_exclude_features = [col for col in exclude_features if col in X.columns]\n",
        "X = X.drop(columns=existing_exclude_features)\n",
        "\n",
        "# Identify numeric and categorical features from the remaining columns\n",
        "numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "object_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# --- Preprocessing Pipeline ---\n",
        "# Create a column transformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False), object_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# --- Train-Test Split ---\n",
        "# Split data into a training set and a final hold-out test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\nData loaded and preprocessed.\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"Number of numeric features: {len(numeric_features)}\")\n",
        "print(f\"Number of categorical features: {len(object_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Pie4D8c9Cw",
        "outputId": "b0fc6d6a-a3a3-45d4-e866-4c3bc9fd3eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Genetic Algorithm for feature selection...\n",
            "gen\tnevals\tavg     \tstd      \tmin     \tmax     \n",
            "0  \t50    \t0.848018\t0.0587701\t0.729148\t0.900256\n",
            "1  \t37    \t0.883321\t0.0326461\t0.743754\t0.90032 \n",
            "2  \t26    \t0.896976\t0.00415124\t0.879885\t0.90032 \n",
            "3  \t33    \t0.897596\t0.00776622\t0.843754\t0.900577\n",
            "4  \t21    \t0.899407\t0.000802464\t0.897694\t0.900833\n",
            "5  \t32    \t0.896947\t0.0209418  \t0.750416\t0.900833\n",
            "6  \t20    \t0.900309\t0.000557694\t0.89795 \t0.901345\n",
            "7  \t27    \t0.900263\t0.0033069  \t0.87745 \t0.902114\n",
            "8  \t30    \t0.897853\t0.0207673  \t0.752659\t0.902434\n",
            "9  \t26    \t0.901318\t0.00050881 \t0.900064\t0.902434\n",
            "10 \t29    \t0.898471\t0.021114   \t0.750865\t0.902691\n",
            "11 \t37    \t0.898315\t0.0210583  \t0.752723\t0.902947\n",
            "12 \t31    \t0.898803\t0.0207623  \t0.753555\t0.902691\n",
            "13 \t30    \t0.901572\t0.00304882 \t0.880525\t0.902691\n",
            "14 \t26    \t0.901488\t0.00359625 \t0.876618\t0.902691\n",
            "15 \t26    \t0.902122\t0.000444688\t0.900512\t0.902691\n",
            "16 \t28    \t0.902228\t0.000517162\t0.899872\t0.903075\n",
            "17 \t36    \t0.902361\t0.000637534\t0.899744\t0.903331\n",
            "18 \t29    \t0.902145\t0.00114038 \t0.898527\t0.903331\n",
            "19 \t24    \t0.896559\t0.0295556  \t0.750993\t0.903459\n",
            "20 \t29    \t0.902867\t0.000759345\t0.898847\t0.903908\n",
            "\n",
            "Genetic Algorithm finished.\n",
            "Number of selected features: 97\n",
            "Selected features: ['time_in_hospital', 'num_medications', 'number_outpatient', 'number_inpatient', 'number_diagnoses', 'mb_admission_grp_1_ct', 'mb_admission_grp_2_ct', 'mb_admission_type_ct', 'distinct_diag_count', 'diag_1_freq', 'LTIS_38_ind', 'LTIS_40_ind', 'LTIS_36_ind', 'LTIS_320_ind', 'CE_430_ind', 'CE_431_ind', 'CMN_162_ind', 'CMN_191_ind', 'CMN_197_ind', 'CMN_199_ind', 'OF_570_ind', 'OF_584_ind', 'OF_277_ind', 'NBD_852_ind', 'STI_806_ind', 'STI_861_ind', 'STI_958_ind', 'OCC_989_ind', 'dx_403_ind_max', 'dx_403_ind_sum', 'dx_707_ind_sum', 'dx_491_ind_max', 'dx_440_ind_max', 'dx_440_ind_sum', 'dx_453_ind_sum', 'dx_571_ind_sum', 'dx_284_ind_max', 'dx_304_ind_sum', 'dx_282_ind_sum', 'dx_332_ind_max', 'dx_332_ind_sum', 'dx_423_ind_max', 'dx_423_ind_sum', 'dx_281_ind_max', 'dx_536_ind_sum', 'dx_368_ind_max', 'dx_368_ind_sum', 'dx_515_ind_max', 'dx_595_ind_max', 'dx_681_ind_sum', 'dx_581_ind_max', 'dx_537_ind_sum', 'dx_490_ind_max', 'dx_583_ind_max', 'dx_583_ind_sum', 'dx_519_ind_max', 'dx_519_ind_sum', 'dx_300_ind_max', 'dx_567_ind_sum', 'dx_E92_ind_max', 'dx_E92_ind_sum', 'dx_514_ind_max', 'dx_494_ind_max', 'dx_042_ind_max', 'dx_404_ind_sum', 'dx_346_ind_max', 'dx_346_ind_sum', 'dx_398_ind_max', 'dx_398_ind_sum', 'dx_753_ind_max', 'dx_753_ind_sum', 'dx_577_ind_sum', 'dx_444_ind_max', 'dx_459_ind_max', 'dx_459_ind_sum', 'dx_790_ind_max', 'dx_337_ind_sum', 'dx_292_ind_max', 'dx_292_ind_sum', 'dx_V42_ind_sum', 'dx_289_ind_max', 'mb_time_in_hospital', 'mb_readmitted_lt30_ct', 'mb_readmitted_no_ct', 'mb_num_lab_procedures_ct', 'mb_num_procedures_ct', 'mb_num_medications_ct', 'mb_number_emergency_ct', 'mb_number_inpatient_ct', 'mb_number_diagnoses_ct', 'A1Cresult', 'change', 'diabetesMed', 'admission_type', 'discharge_disposition', 'admission_source', 'diagnosis_2']\n",
            "\n",
            "Data preprocessed with selected features. New input dimension for CNN: 764\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# --- Custom Transformer for Data Type Conversion ---\n",
        "# This helper class ensures that all data passed to the OneHotEncoder is of string type,\n",
        "# preventing errors with mixed types (e.g., columns containing both numbers and strings).\n",
        "class TypeConverter(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        return X.astype(str)\n",
        "\n",
        "# --- Genetic Algorithm Setup ---\n",
        "\n",
        "# The number of features to select from\n",
        "# We determine this by combining the numeric and object feature lists from the previous cell.\n",
        "N_FEATURES = len(numeric_features) + len(object_features)\n",
        "\n",
        "# Define the fitness and individual creation for the GA\n",
        "# We want to maximize accuracy, so weights=(1.0,)\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "# An individual is a list of bits (0 or 1) with a fitness attribute\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Attribute generator: a gene is either 0 (not selected) or 1 (selected)\n",
        "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
        "\n",
        "# Individual initializer: creates an individual with N_FEATURES genes\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=N_FEATURES)\n",
        "\n",
        "# Population initializer\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# --- Fitness Evaluation Function ---\n",
        "# This function determines how \"good\" a feature subset is by training a simple model.\n",
        "def evaluate_features(individual):\n",
        "    # Get indices of selected features (where the gene is 1)\n",
        "    selected_indices = [i for i, bit in enumerate(individual) if bit == 1]\n",
        "\n",
        "    # If the GA creates an individual with no features, its fitness is 0.\n",
        "    if not selected_indices:\n",
        "        return 0.0,\n",
        "\n",
        "    # Combine all feature names to create a master list\n",
        "    all_feature_names = np.array(numeric_features + object_features)\n",
        "    # Select the feature names based on the individual's genes\n",
        "    selected_features = all_feature_names[selected_indices].tolist()\n",
        "\n",
        "    # Separate the selected features back into numeric and categorical types\n",
        "    sel_numeric = [f for f in selected_features if f in numeric_features]\n",
        "    sel_categorical = [f for f in selected_features if f in object_features]\n",
        "\n",
        "    # Create a nested pipeline for categorical features to first convert them to strings\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('tostring', TypeConverter()),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
        "    ])\n",
        "\n",
        "    # Create a temporary pipeline with a preprocessor for ONLY the selected features\n",
        "    # This is crucial to evaluate the specific subset.\n",
        "    pipeline = Pipeline([\n",
        "        ('col_transformer', ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', MinMaxScaler(), sel_numeric),\n",
        "                ('cat', categorical_transformer, sel_categorical)\n",
        "            ],\n",
        "            remainder='drop' # Drop any columns that were not selected\n",
        "        )),\n",
        "        # Use a fast model like Logistic Regression for quick fitness evaluation\n",
        "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    # For speed, we train and evaluate on a smaller sample of the data.\n",
        "    X_sample, _, y_sample, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42, stratify=y_train)\n",
        "    pipeline.fit(X_sample, y_sample)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # DEAP requires the fitness to be a tuple\n",
        "    return accuracy,\n",
        "\n",
        "# --- Register GA Operators ---\n",
        "toolbox.register(\"evaluate\", evaluate_features)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint) # Crossover operator\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05) # Mutation operator\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3) # Selection operator\n",
        "\n",
        "# --- Run the Genetic Algorithm ---\n",
        "print(\"Starting Genetic Algorithm for feature selection...\")\n",
        "population = toolbox.population(n=50)\n",
        "NGEN = 20 # Number of generations to run\n",
        "CXPB, MUTPB = 0.5, 0.2 # Crossover and mutation probabilities\n",
        "\n",
        "# Use a Hall of Fame to keep track of the single best individual found\n",
        "hof = tools.HallOfFame(1)\n",
        "\n",
        "# Set up statistics to track during evolution\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"std\", np.std)\n",
        "stats.register(\"min\", np.min)\n",
        "stats.register(\"max\", np.max)\n",
        "\n",
        "# Run the evolution process\n",
        "algorithms.eaSimple(population, toolbox, cxpb=CXPB, mutpb=MUTPB, ngen=NGEN,\n",
        "                    stats=stats, halloffame=hof, verbose=True)\n",
        "\n",
        "# --- Get the Best Features from the Hall of Fame ---\n",
        "best_individual = hof[0]\n",
        "all_feature_names = np.array(numeric_features + object_features)\n",
        "selected_features_mask = np.array(best_individual).astype(bool)\n",
        "selected_features = all_feature_names[selected_features_mask].tolist()\n",
        "\n",
        "print(\"\\nGenetic Algorithm finished.\")\n",
        "print(f\"Number of selected features: {len(selected_features)}\")\n",
        "print(f\"Selected features: {selected_features}\")\n",
        "\n",
        "# --- Final Preprocessing with Selected Features ---\n",
        "# Create the final preprocessor that will be used to prepare data for the CNN\n",
        "sel_numeric_final = [f for f in selected_features if f in numeric_features]\n",
        "sel_categorical_final = [f for f in selected_features if f in object_features]\n",
        "\n",
        "# The categorical transformer pipeline to ensure data is string before one-hot encoding\n",
        "final_categorical_transformer = Pipeline(steps=[\n",
        "    ('tostring', TypeConverter()),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))\n",
        "])\n",
        "\n",
        "final_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), sel_numeric_final),\n",
        "        ('cat', final_categorical_transformer, sel_categorical_final)\n",
        "    ],\n",
        "    remainder='drop' # IMPORTANT: Drop columns not selected by the GA to prevent data type errors\n",
        ")\n",
        "\n",
        "# Transform the training and test sets using the final preprocessor\n",
        "X_train_selected = final_preprocessor.fit_transform(X_train)\n",
        "X_test_selected = final_preprocessor.transform(X_test)\n",
        "\n",
        "# The number of input features for our CNN is now the number of columns in the processed data\n",
        "INPUT_SIZE = X_train_selected.shape[1]\n",
        "\n",
        "print(f\"\\nData preprocessed with selected features. New input dimension for CNN: {INPUT_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaQjMP_SdoTm",
        "outputId": "ff96355f-2387-4468-f973-38a71bf71c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch 1D CNN model defined.\n"
          ]
        }
      ],
      "source": [
        "class SingleShotCNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_filters, kernel_size, dropout_p):\n",
        "        super(SingleShotCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layer\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, padding='same')\n",
        "        self.relu = nn.ReLU()\n",
        "        # Global Max Pooling will reduce the dimension to (batch_size, num_filters)\n",
        "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # The input to the linear layer is num_filters because of global max pooling\n",
        "        self.fc1 = nn.Linear(num_filters, 50)\n",
        "        self.fc2 = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input for Conv1d: (batch_size, channels, sequence_length)\n",
        "        # Our features are the sequence, and we have 1 channel.\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten the output for the fully connected layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(\"PyTorch 1D CNN model defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCUKX_OHh6ZF",
        "outputId": "2c431bb9-7e68-4772-a1ec-822328eb47f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna objective function defined.\n"
          ]
        }
      ],
      "source": [
        "def objective(trial):\n",
        "    # --- Hyperparameter Search Space ---\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    num_filters = trial.suggest_int(\"num_filters\", 16, 64, step=16)\n",
        "    kernel_size = trial.suggest_int(\"kernel_size\", 3, 7, step=2)\n",
        "    dropout_p = trial.suggest_float(\"dropout_p\", 0.1, 0.5)\n",
        "    epochs = 15 # Fixed number of epochs for each trial for faster tuning\n",
        "    \n",
        "    print(f\"\\n--- Starting Trial {trial.number} ---\")\n",
        "    # --- K-Fold Cross-Validation ---\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_accuracies = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_selected, y_train)):\n",
        "        # Data for this fold\n",
        "        X_train_fold, X_val_fold = X_train_selected[train_idx], X_train_selected[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        # Convert to PyTorch Tensors, ensuring data is float32 to prevent errors\n",
        "        X_train_tensor = torch.FloatTensor(X_train_fold.astype(np.float32))\n",
        "        y_train_tensor = torch.LongTensor(y_train_fold)\n",
        "        X_val_tensor = torch.FloatTensor(X_val_fold.astype(np.float32))\n",
        "        y_val_tensor = torch.LongTensor(y_val_fold)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        # --- Model Training and Evaluation for the Fold ---\n",
        "        model = SingleShotCNN(INPUT_SIZE, num_classes=2, num_filters=num_filters, kernel_size=kernel_size, dropout_p=dropout_p).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"    Epoch {epoch + 1}/{epochs}\", end='\\r')\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X = batch_X.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        print(f\"    Fold {fold + 1} completed.\")\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_val_tensor = X_val_tensor.to(device)\n",
        "            val_outputs = model(X_val_tensor)\n",
        "            _, predicted = torch.max(val_outputs.data, 1)\n",
        "            accuracy = accuracy_score(y_val_tensor.numpy(),  predicted.cpu().numpy())\n",
        "            fold_accuracies.append(accuracy)\n",
        "            print(f\"    Fold Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Return the mean accuracy across all folds\n",
        "    mean_accuracy = np.mean(fold_accuracies)\n",
        "    print(f\"--- Trial {trial.number} Finished | Mean Accuracy: {mean_accuracy:.4f} ---\")\n",
        "    return np.mean(fold_accuracies)\n",
        "\n",
        "print(\"Optuna objective function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOQRnnl4h8-j",
        "outputId": "dac636fd-aaf4-4123-b6fc-d9d815f70701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Optuna hyperparameter search...\n",
            "\n",
            "--- Starting Trial 0 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7578\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7497\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7569\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7565\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7517\n",
            "--- Trial 0 Finished | Mean Accuracy: 0.7545 ---\n",
            "\n",
            "--- Starting Trial 1 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7490\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7457\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7497\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7549\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7436\n",
            "--- Trial 1 Finished | Mean Accuracy: 0.7486 ---\n",
            "\n",
            "--- Starting Trial 2 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.5421\n",
            "--- Trial 2 Finished | Mean Accuracy: 0.5420 ---\n",
            "\n",
            "--- Starting Trial 3 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.6517\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7112\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7018\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7007\n",
            "--- Trial 3 Finished | Mean Accuracy: 0.6615 ---\n",
            "\n",
            "--- Starting Trial 4 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.5492\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.5421\n",
            "--- Trial 4 Finished | Mean Accuracy: 0.5435 ---\n",
            "\n",
            "--- Starting Trial 5 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.6453\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.6859\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7046\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7084\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.6792\n",
            "--- Trial 5 Finished | Mean Accuracy: 0.6847 ---\n",
            "\n",
            "--- Starting Trial 6 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.5421\n",
            "--- Trial 6 Finished | Mean Accuracy: 0.5420 ---\n",
            "\n",
            "--- Starting Trial 7 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.5421\n",
            "--- Trial 7 Finished | Mean Accuracy: 0.5420 ---\n",
            "\n",
            "--- Starting Trial 8 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.5421\n",
            "--- Trial 8 Finished | Mean Accuracy: 0.5420 ---\n",
            "\n",
            "--- Starting Trial 9 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.6984\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.6855\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7011\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7189\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.6090\n",
            "--- Trial 9 Finished | Mean Accuracy: 0.6826 ---\n",
            "\n",
            "--- Starting Trial 10 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7303\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7258\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7361\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7197\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7248\n",
            "--- Trial 10 Finished | Mean Accuracy: 0.7274 ---\n",
            "\n",
            "--- Starting Trial 11 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7484\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7333\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7402\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7538\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7435\n",
            "--- Trial 11 Finished | Mean Accuracy: 0.7438 ---\n",
            "\n",
            "--- Starting Trial 12 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7181\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7374\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7289\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7283\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7287\n",
            "--- Trial 12 Finished | Mean Accuracy: 0.7283 ---\n",
            "\n",
            "--- Starting Trial 13 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7145\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.6923\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7283\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7365\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7423\n",
            "--- Trial 13 Finished | Mean Accuracy: 0.7228 ---\n",
            "\n",
            "--- Starting Trial 14 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7205\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7240\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7273\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7316\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7135\n",
            "--- Trial 14 Finished | Mean Accuracy: 0.7234 ---\n",
            "\n",
            "--- Starting Trial 15 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7106\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.6907\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7136\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7232\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7000\n",
            "--- Trial 15 Finished | Mean Accuracy: 0.7076 ---\n",
            "\n",
            "--- Starting Trial 16 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.5420\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.5421\n",
            "--- Trial 16 Finished | Mean Accuracy: 0.5420 ---\n",
            "\n",
            "--- Starting Trial 17 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.6886\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.6876\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.5926\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7143\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7143\n",
            "--- Trial 17 Finished | Mean Accuracy: 0.6795 ---\n",
            "\n",
            "--- Starting Trial 18 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7522\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7565\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7561\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7514\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7481\n",
            "--- Trial 18 Finished | Mean Accuracy: 0.7528 ---\n",
            "\n",
            "--- Starting Trial 19 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7481\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7470\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7387\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7452\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7401\n",
            "--- Trial 19 Finished | Mean Accuracy: 0.7438 ---\n",
            "\n",
            "--- Starting Trial 20 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7128\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.6393\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7212\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7229\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7216\n",
            "--- Trial 20 Finished | Mean Accuracy: 0.7035 ---\n",
            "\n",
            "--- Starting Trial 21 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7548\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7523\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7481\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7498\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7465\n",
            "--- Trial 21 Finished | Mean Accuracy: 0.7503 ---\n",
            "\n",
            "--- Starting Trial 22 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7530\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7486\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7481\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7522\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7497\n",
            "--- Trial 22 Finished | Mean Accuracy: 0.7503 ---\n",
            "\n",
            "--- Starting Trial 23 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7478\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7458\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7519\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7489\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7469\n",
            "--- Trial 23 Finished | Mean Accuracy: 0.7483 ---\n",
            "\n",
            "--- Starting Trial 24 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7481\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7506\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7520\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7505\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7450\n",
            "--- Trial 24 Finished | Mean Accuracy: 0.7492 ---\n",
            "\n",
            "--- Starting Trial 25 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7415\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7421\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7437\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7426\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7392\n",
            "--- Trial 25 Finished | Mean Accuracy: 0.7418 ---\n",
            "\n",
            "--- Starting Trial 26 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7084\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7223\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7136\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7227\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7109\n",
            "--- Trial 26 Finished | Mean Accuracy: 0.7156 ---\n",
            "\n",
            "--- Starting Trial 27 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7506\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7515\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7518\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7409\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7477\n",
            "--- Trial 27 Finished | Mean Accuracy: 0.7485 ---\n",
            "\n",
            "--- Starting Trial 28 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7045\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.6898\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.6923\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7246\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7092\n",
            "--- Trial 28 Finished | Mean Accuracy: 0.7041 ---\n",
            "\n",
            "--- Starting Trial 29 ---\n",
            "    Fold 1 completed.\n",
            "    Fold Accuracy: 0.7518\n",
            "    Fold 2 completed.\n",
            "    Fold Accuracy: 0.7515\n",
            "    Fold 3 completed.\n",
            "    Fold Accuracy: 0.7542\n",
            "    Fold 4 completed.\n",
            "    Fold Accuracy: 0.7567\n",
            "    Fold 5 completed.\n",
            "    Fold Accuracy: 0.7495\n",
            "--- Trial 29 Finished | Mean Accuracy: 0.7528 ---\n",
            "\n",
            "Optuna study finished.\n",
            "\n",
            "--- Best Results ---\n",
            "Best Accuracy: 0.7545\n",
            "Best Hyperparameters:\n",
            "  lr: 0.002545768273574632\n",
            "  optimizer: Adam\n",
            "  num_filters: 64\n",
            "  kernel_size: 7\n",
            "  dropout_p: 0.20018027811185532\n",
            "\n",
            "Training final model with best hyperparameters on all training data...\n",
            "\n",
            "--- Final Model Performance ---\n",
            "Accuracy on unseen Test Set: 0.7575\n"
          ]
        }
      ],
      "source": [
        "# Create and run the Optuna study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "print(\"Starting Optuna hyperparameter search...\")\n",
        "\n",
        "# Increase n_trials for a more thorough search, e.g., 50 or 100\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "print(\"\\nOptuna study finished.\")\n",
        "\n",
        "# --- Display Results ---\n",
        "best_trial = study.best_trial\n",
        "print(\"\\n--- Best Results ---\")\n",
        "print(f\"Best Accuracy: {best_trial.value:.4f}\")\n",
        "print(\"Best Hyperparameters:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# --- Final Evaluation ---\n",
        "# Now, we train the final model on the FULL training data with the best hyperparameters\n",
        "# and evaluate on the unseen test set.\n",
        "\n",
        "print(\"\\nTraining final model with best hyperparameters on all training data...\")\n",
        "best_params = best_trial.params\n",
        "final_model = SingleShotCNN(\n",
        "    INPUT_SIZE,\n",
        "    num_classes=2,\n",
        "    num_filters=best_params['num_filters'],\n",
        "    kernel_size=best_params['kernel_size'],\n",
        "    dropout_p=best_params['dropout_p']\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = getattr(optim, best_params['optimizer'])(final_model.parameters(), lr=best_params['lr'])\n",
        "\n",
        "# Create final dataloaders\n",
        "X_train_tensor = torch.FloatTensor(X_train_selected)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test_selected)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "final_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "final_train_loader = DataLoader(final_train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Final training loop\n",
        "final_model.train()\n",
        "for epoch in range(25): # Train for a few more epochs on the full data\n",
        "    for batch_X, batch_y in final_train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = final_model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Final evaluation on the hold-out test set\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    # 3. Move data tensor to the device\n",
        "    test_outputs = final_model(X_test_tensor.to(device))\n",
        "    _, predicted = torch.max(test_outputs.data, 1)\n",
        "    # Move predicted tensor back to cpu for numpy conversion\n",
        "    final_accuracy = accuracy_score(y_test_tensor.numpy(), predicted.cpu().numpy())\n",
        "\n",
        "print(\"\\n--- Final Model Performance ---\")\n",
        "print(f\"Accuracy on unseen Test Set: {final_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
